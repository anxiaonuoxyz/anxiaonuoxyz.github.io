<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css" integrity="sha256-2H3fkXt6FEmrReK448mDVGKb3WW2ZZw35gI7vqHOE4Y=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Muse","version":"8.6.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>
<meta name="description" content="todo跑一下结果复现一下和qmix对比 看一下sc2的接口中action 6 - 29编码是如何定义的？最近的或者？ 结合两个论文的图画一个整图 问题3： 每个agent已经选好action和role之后的输出进入qmix 每个agent选的分别是$Q_1,Q_2$等  问题2： 1234567891011121314151617enemy_units &#x3D; [    unit    for un">
<meta property="og:type" content="article">
<meta property="og:title" content="code for rode">
<meta property="og:url" content="http://example.com/2021/09/04/Code%20for%20rode/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="todo跑一下结果复现一下和qmix对比 看一下sc2的接口中action 6 - 29编码是如何定义的？最近的或者？ 结合两个论文的图画一个整图 问题3： 每个agent已经选好action和role之后的输出进入qmix 每个agent选的分别是$Q_1,Q_2$等  问题2： 1234567891011121314151617enemy_units &#x3D; [    unit    for un">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://i.loli.net/2021/08/29/6SchVMgw3ynokWL.png">
<meta property="og:image" content="https://i.loli.net/2021/08/24/l3KIXckob5UnTg7.png">
<meta property="og:image" content="https://i.loli.net/2021/08/12/8qzBxmn1EjhYQi4.png">
<meta property="og:image" content="https://i.loli.net/2021/07/18/3T1RMwCdmQHzAn8.png">
<meta property="article:published_time" content="2021-09-04T07:03:50.448Z">
<meta property="article:modified_time" content="2021-09-04T07:32:46.785Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2021/08/29/6SchVMgw3ynokWL.png">


<link rel="canonical" href="http://example.com/2021/09/04/Code%20for%20rode/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://example.com/2021/09/04/Code%20for%20rode/","path":"2021/09/04/Code for rode/","title":"code for rode"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>code for rode | Hexo</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Hexo</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>







</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#todo"><span class="nav-number">1.</span> <span class="nav-text">todo</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0"><span class="nav-number">2.</span> <span class="nav-text">实现</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-overview">
            <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">4</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>



          </div>
        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/09/04/Code%20for%20rode/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          code for rode
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2021-09-04 15:03:50 / Modified: 15:32:46" itemprop="dateCreated datePublished" datetime="2021-09-04T15:03:50+08:00">2021-09-04</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="todo"><a href="#todo" class="headerlink" title="todo"></a>todo</h1><p>跑一下结果复现一下和qmix对比</p>
<p>看一下sc2的接口中action 6 - 29编码是如何定义的？最近的或者？</p>
<p>结合两个论文的图画一个整图</p>
<p>问题3：</p>
<p>每个agent已经选好action和role之后的输出进入qmix 每个agent选的分别是$Q_1,Q_2$等</p>
<p><img src="https://i.loli.net/2021/08/29/6SchVMgw3ynokWL.png" alt="rode"></p>
<p>问题2：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">enemy_units = [</span><br><span class="line">    unit</span><br><span class="line">    <span class="keyword">for</span> unit <span class="keyword">in</span> self._obs.observation.raw_data.units<span class="comment">#从环境中获取</span></span><br><span class="line">    <span class="keyword">if</span> unit.owner == <span class="number">2</span></span><br><span class="line">]</span><br><span class="line">enemy_units_sorted = <span class="built_in">sorted</span>(<span class="comment">#按照单位类型 pos.x pos.y对其进行排序</span></span><br><span class="line">    enemy_units,</span><br><span class="line">    key=attrgetter(<span class="string">&quot;unit_type&quot;</span>, <span class="string">&quot;pos.x&quot;</span>, <span class="string">&quot;pos.y&quot;</span>),</span><br><span class="line">    reverse=<span class="literal">False</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(enemy_units_sorted)):</span><br><span class="line">    enemy_unit = enemy_units_sorted[i]</span><br><span class="line">    self.enemies[i] = enemy_unit<span class="comment">#self.enemies</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><p>服务器需要有git</p>
<p>用205导出的环境 python3.7 torch1.2.0 vision0.4.0</p>
<p>run里面 抽样数据前需要改一下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#如果verbose为true 返回值为俩</span></span><br><span class="line"><span class="keyword">if</span> args.verbose:</span><br><span class="line">    episode_batch = runner.run(test_mode=<span class="literal">False</span>)[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    episode_batch = runner.run(test_mode=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>信息用log</p>
<p>replay存储 改startcraft.py文件没有用 要改config中的sc2.yaml，相对路径是在sc游戏目录replay下的</p>
<p>运行</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup python src/main.py --config=rode --env-config=sc2 with env_args.map_name=corridor n_role_clusters=3 role_interval=5 t_max=5050000  &gt; runoob_testeval_zuihou.log 2&gt;&amp;1  </span><br></pre></td></tr></table></figure>
<p>测试并且存储replay命令 checkpoint_path为测试模型</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup python src/main.py --config=rode --env-config=sc2 with env_args.map_name=corridor evaluate=True checkpoint_path=&quot;/home/gengyaojun/qlt/RODE-main/results/models/rode__2021-08-21_13-17-15/&quot; save_replay=True &gt; runoob_testeval_zuihou.log 2&gt;&amp;1  </span><br></pre></td></tr></table></figure>
<p>verbose=true有pic_replay 但是运行有bug</p>
<p>角色空间更新</p>
<p><img src="https://i.loli.net/2021/08/24/l3KIXckob5UnTg7.png" alt="image-20210824104756838"></p>
<p>构建网络及初始化 </p>
<p>​    构建rode_controller</p>
<p>​        构建agent以及role policy</p>
<p>​        构建动作选择器</p>
<p>​        构建角色选择器</p>
<p>​        构建动作编码器</p>
<p>​    构建episode-runner</p>
<p>​    构建learner</p>
<p>获取episode数据</p>
<p>​    获取一个batch里所有agent的action</p>
<p>​        根据role的q值选择role</p>
<p>​        计算role中不同的action的q值</p>
<p>​        从role中可用的action中选action</p>
<p>​    更新信息</p>
<p>抽样数据进行训练</p>
<p>​    训练q值</p>
<p>​        计算chosen_action_qvals 和chosen_role_qvals </p>
<p>​        得到对应的target</p>
<p>​        将chosen_action_qvals 和chosen_role_qvals 以及他们target经过qmix</p>
<p>​        根据公式计算td-error</p>
<p>​        计算loss 训练</p>
<p>​    训练动作编码器</p>
<p>​        经过网络计算obs和reward</p>
<p>​        通过公式计算</p>
<p>​        得到loss 训练</p>
<p>​    更新role_action空间</p>
<p>​        </p>
<p>​    </p>
<p>执行命令</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 src/main.py --config=rode --env-config=sc2 with env_args.map_name=corridor n_role_clusters=3 role_interval=5 t_max=5050000</span><br></pre></td></tr></table></figure>
<p>main.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用sacred跟踪记录实验</span></span><br><span class="line">ex = Experiment(<span class="string">&quot;pymarl&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#获取命令行参数</span></span><br><span class="line">params = deepcopy(sys.argv)</span><br><span class="line"><span class="comment"># Load algorithm and env base configs</span></span><br><span class="line">env_config = _get_config(params, <span class="string">&quot;--env-config&quot;</span>, <span class="string">&quot;envs&quot;</span>)</span><br><span class="line">alg_config = _get_config(params, <span class="string">&quot;--config&quot;</span>, <span class="string">&quot;algs&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># now add all the config to sacred</span></span><br><span class="line"><span class="comment">#sacred记录参数</span></span><br><span class="line">ex.add_config(config_dict)<span class="comment">#向此Experiment添加配置条目</span></span><br><span class="line"><span class="comment">#The FileStorageObserver is the most basic observer and requires the least amount of setup</span></span><br><span class="line">ex.observers.append(FileStorageObserver.create(file_obs_path))<span class="comment">#配置Observer</span></span><br><span class="line"><span class="comment">#sacred</span></span><br><span class="line">ex.run_commandline(params)<span class="comment">#运行本实验的命令行界面。</span></span><br></pre></td></tr></table></figure>
<p><img src="https://i.loli.net/2021/08/12/8qzBxmn1EjhYQi4.png" alt="image-20210812112944350"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#用@ex.main 修饰的函数是实验的主要函数。 如果运行实验，它就会被执行，它也用于确定实验的源文件。</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@ex.main</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_main</span>(<span class="params">_run, _config, _log</span>):</span></span><br><span class="line">    <span class="comment"># Setting the random seed throughout the modules</span></span><br><span class="line">    config = config_copy(_config)</span><br><span class="line">    np.random.seed(config[<span class="string">&quot;seed&quot;</span>])</span><br><span class="line">    th.manual_seed(config[<span class="string">&quot;seed&quot;</span>])</span><br><span class="line">    config[<span class="string">&#x27;env_args&#x27;</span>][<span class="string">&#x27;seed&#x27;</span>] = config[<span class="string">&quot;seed&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># run the framework</span></span><br><span class="line">    run(_run, config, _log)<span class="comment">#运行</span></span><br></pre></td></tr></table></figure>
<p>run.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br></pre></td><td class="code"><pre><span class="line">_config = args_sanity_check(_config, _log)<span class="comment">#检查是否出错 cuda是否可用 根据batch_size_run调整test_nepisode</span></span><br><span class="line">args = SN(**_config)<span class="comment">#使用SimpleNamespace使得字典可以通过a.b访问</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#配置logging scared以及加载参数</span></span><br><span class="line"></span><br><span class="line">run_sequential(args=args, logger=logger)<span class="comment">#运行</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_sequential</span>(<span class="params">args, logger</span>):</span></span><br><span class="line">    <span class="comment"># Init运行器，这样我们就能得到env信息</span></span><br><span class="line">    <span class="comment">#from runners import REGISTRY as r_REGISTRY</span></span><br><span class="line">    runner = r_REGISTRY[args.runner](args=args, logger=logger)<span class="comment">#args.runner=episode</span></span><br><span class="line">    <span class="comment">#episode_runner.py</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, args, logger</span>):</span></span><br><span class="line">            self.args = args</span><br><span class="line">            self.logger = logger</span><br><span class="line">            self.batch_size = self.args.batch_size_run</span><br><span class="line">            <span class="keyword">assert</span> self.batch_size == <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            self.env = env_REGISTRY[self.args.env](**self.args.env_args)<span class="comment">#确定了环境为sc</span></span><br><span class="line">            self.episode_limit = self.env.episode_limit</span><br><span class="line">            self.t = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            self.t_env = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            self.train_returns = []</span><br><span class="line">            self.test_returns = []</span><br><span class="line">            self.train_stats = &#123;&#125;</span><br><span class="line">            self.test_stats = &#123;&#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Log the first run</span></span><br><span class="line">            self.log_train_stats_t = -<span class="number">1000000</span></span><br><span class="line"></span><br><span class="line">            self.verbose = args.verbose</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#初始画一些信息 scheme groups preprocess</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 设置多用户控制器</span></span><br><span class="line">    <span class="comment">#from controllers import REGISTRY as mac_REGISTRY</span></span><br><span class="line">    mac = mac_REGISTRY[args.mac](buffer.scheme, groups, args)</span><br><span class="line">    <span class="comment">#rode_controller.py</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, scheme, groups, args</span>):</span></span><br><span class="line">        self.n_agents = args.n_agents</span><br><span class="line">        self.n_actions = args.n_actions</span><br><span class="line">        self.args = args</span><br><span class="line">        self.role_interval = args.role_interval</span><br><span class="line"></span><br><span class="line">        input_shape = self._get_input_shape(scheme)</span><br><span class="line">        self._build_agents(input_shape)<span class="comment">#创建agent</span></span><br><span class="line">            <span class="function"><span class="keyword">def</span> <span class="title">_build_agents</span>(<span class="params">self, input_shape</span>):</span></span><br><span class="line">        		self.agent = agent_REGISTRY[self.args.agent](input_shape, self.args)</span><br><span class="line">                self.role_agent = agent_REGISTRY[self.args.agent](input_shape, self.args)</span><br><span class="line">                </span><br><span class="line">                	<span class="keyword">from</span> modules.agents <span class="keyword">import</span> REGISTRY <span class="keyword">as</span> agent_REGISTRY</span><br><span class="line">                	<span class="class"><span class="keyword">class</span> <span class="title">RODEAgent</span>(<span class="params">nn.Module</span>):</span><span class="comment">#一个linear relu接一个gru 图B下半部分</span></span><br><span class="line">    					<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, input_shape, args</span>):</span></span><br><span class="line">                            <span class="built_in">super</span>(RODEAgent, self).__init__()</span><br><span class="line">                            self.args = args</span><br><span class="line"></span><br><span class="line">                        self.fc1 = nn.Linear(input_shape, args.rnn_hidden_dim)</span><br><span class="line">                        self.rnn = nn.GRUCell(args.rnn_hidden_dim, args.rnn_hidden_dim)</span><br><span class="line"></span><br><span class="line">                        <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs, hidden_state</span>):</span></span><br><span class="line">                            x = F.relu(self.fc1(inputs))</span><br><span class="line">                            h_in = hidden_state.reshape(-<span class="number">1</span>, self.args.rnn_hidden_dim)</span><br><span class="line">                            h = self.rnn(x, h_in)</span><br><span class="line">                            <span class="keyword">return</span> h</span><br><span class="line"></span><br><span class="line">        self.n_roles = <span class="number">3</span></span><br><span class="line">        self._build_roles()</span><br><span class="line">        </span><br><span class="line">            <span class="function"><span class="keyword">def</span> <span class="title">_build_roles</span>(<span class="params">self</span>):</span><span class="comment">#创建role</span></span><br><span class="line">        		self.roles = [role_REGISTRY[self.args.role](self.args) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.n_roles)]</span><br><span class="line">            <span class="class"><span class="keyword">class</span> <span class="title">DotRole</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">            <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, args</span>):</span></span><br><span class="line">                <span class="built_in">super</span>(DotRole, self).__init__()</span><br><span class="line">                self.args = args</span><br><span class="line">                self.n_actions = args.n_actions</span><br><span class="line"></span><br><span class="line">                self.q_fc = nn.Linear(args.rnn_hidden_dim, args.action_latent_dim)<span class="comment">#图c的角色策略 一个没有激活函数的线性层</span></span><br><span class="line">                self.action_space = th.ones(args.n_actions).to(args.device)</span><br><span class="line"></span><br><span class="line">            <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, h, action_latent</span>):</span></span><br><span class="line">                role_key = self.q_fc(h)  <span class="comment"># [bs, action_latent] [n_actions, action_latent]</span></span><br><span class="line">                role_key = role_key.unsqueeze(-<span class="number">1</span>)</span><br><span class="line">                action_latent_reshaped = action_latent.unsqueeze(<span class="number">0</span>).repeat(role_key.shape[<span class="number">0</span>], <span class="number">1</span>, <span class="number">1</span>)<span class="comment">#repeat 默认axis=0 第一行元素额重复role_key.shape[0]次 第二行重复1次 第三行重复1次</span></span><br><span class="line">                <span class="comment">#unsqueeze 在第n维上增加一个维度</span></span><br><span class="line">                <span class="comment">#action_latent_reshaped实际上是role_key.shape[0]个action_latent叠加</span></span><br><span class="line">                q = th.bmm(action_latent_reshaped, role_key).squeeze()<span class="comment">#最右边的q</span></span><br><span class="line">                <span class="keyword">return</span> q    </span><br><span class="line">                </span><br><span class="line">                </span><br><span class="line">        self.agent_output_type = args.agent_output_type</span><br><span class="line"></span><br><span class="line">        self.action_selector = action_REGISTRY[args.action_selector](args)</span><br><span class="line">        <span class="comment">#多项行动选择器</span></span><br><span class="line"></span><br><span class="line">---------------------------------------------------------------------------------</span><br><span class="line"> action_selector 开始</span><br><span class="line">			<span class="keyword">from</span> components.action_selectors <span class="keyword">import</span> REGISTRY <span class="keyword">as</span> action_REGISTRY</span><br><span class="line">            </span><br><span class="line">            <span class="comment">#action_selector: &quot;soft_epsilon_greedy&quot;</span></span><br><span class="line">            <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, args</span>):</span></span><br><span class="line">        	self.args = args</span><br><span class="line"></span><br><span class="line">            self.schedule = DecayThenFlatSchedule(args.epsilon_start, args.epsilon_finish, args.epsilon_anneal_time,args.epsilon_anneal_time_exp,args.role_action_spaces_update_start,decay=<span class="string">&quot;linear&quot;</span>)<span class="comment">#epsilon衰减的时间表</span></span><br><span class="line">            self.epsilon = self.schedule.<span class="built_in">eval</span>(<span class="number">0</span>)<span class="comment">#返回的是一个值</span></span><br><span class="line">            </span><br><span class="line"> action_selector 结束</span><br><span class="line">---------------------------------------------------------------------------------</span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">        self.role_selector = role_selector_REGISTRY[args.role_selector](input_shape, args)</span><br><span class="line">        <span class="comment">#角色选择器 图B上面</span></span><br><span class="line">        <span class="class"><span class="keyword">class</span> <span class="title">DotSelector</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">            <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, input_shape, args</span>):</span></span><br><span class="line">                <span class="built_in">super</span>(DotSelector, self).__init__()</span><br><span class="line">                self.args = args</span><br><span class="line">                self.epsilon_start = self.args.epsilon_start</span><br><span class="line">                self.epsilon_finish = self.args.role_epsilon_finish</span><br><span class="line">                self.epsilon_anneal_time = self.args.epsilon_anneal_time</span><br><span class="line">                self.epsilon_anneal_time_exp = self.args.epsilon_anneal_time_exp</span><br><span class="line">                self.delta = (self.epsilon_start - self.epsilon_finish) / self.epsilon_anneal_time</span><br><span class="line">                self.role_action_spaces_update_start = self.args.role_action_spaces_update_start</span><br><span class="line">                self.epsilon_start_t = <span class="number">0</span></span><br><span class="line">                self.epsilon_reset = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">                self.fc1 = nn.Linear(args.rnn_hidden_dim, <span class="number">2</span> * args.rnn_hidden_dim)</span><br><span class="line">                self.fc2 = nn.Linear(<span class="number">2</span> * args.rnn_hidden_dim, args.action_latent_dim)</span><br><span class="line"></span><br><span class="line">                self.epsilon = <span class="number">0.05</span></span><br><span class="line">                </span><br><span class="line">            <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs, role_latent</span>):</span></span><br><span class="line">                x = self.fc2(F.relu(self.fc1(inputs)))  <span class="comment"># [bs, action_dim] [n_roles, action_dim] (bs may be bs*n_agents)</span></span><br><span class="line">                x = x.unsqueeze(-<span class="number">1</span>)</span><br><span class="line">                role_latent_reshaped = role_latent.unsqueeze(<span class="number">0</span>).repeat(x.shape[<span class="number">0</span>], <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">                role_q = th.bmm(role_latent_reshaped, x).squeeze()<span class="comment">#图B上面role_repre角色表征和经过两层线性层的角色选择器的结果相乘</span></span><br><span class="line">                <span class="keyword">return</span> role_q</span><br><span class="line">                </span><br><span class="line">        self.action_encoder = action_encoder_REGISTRY[args.action_encoder](args)</span><br><span class="line">        <span class="comment">#动作编码器 图a</span></span><br><span class="line">        <span class="class"><span class="keyword">class</span> <span class="title">ObsRewardEncoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">            <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, args</span>):</span></span><br><span class="line">                <span class="built_in">super</span>(ObsRewardEncoder, self).__init__()</span><br><span class="line">                self.args = args</span><br><span class="line">                self.n_agents = args.n_agents</span><br><span class="line">                self.n_actions = args.n_actions</span><br><span class="line">                self.mixing_embed_dim = args.mixing_embed_dim</span><br><span class="line">                self.action_latent_dim = args.action_latent_dim</span><br><span class="line"></span><br><span class="line">                self.state_dim = <span class="built_in">int</span>(np.prod(args.state_shape))</span><br><span class="line">                self.obs_dim = <span class="built_in">int</span>(np.prod(args.obs_shape))</span><br><span class="line"></span><br><span class="line">                <span class="comment">#注意维度</span></span><br><span class="line">                <span class="comment">#中间那个大编码器</span></span><br><span class="line">                self.obs_encoder_avg = nn.Sequential(</span><br><span class="line">                    nn.Linear(self.obs_dim + self.n_actions * (self.n_agents - <span class="number">1</span>), args.state_latent_dim * <span class="number">2</span>),</span><br><span class="line">                    nn.ReLU(),</span><br><span class="line">                    nn.Linear(args.state_latent_dim * <span class="number">2</span>, args.state_latent_dim))</span><br><span class="line">                <span class="comment">#左边最上面那个解码器 出来obs</span></span><br><span class="line">                self.obs_decoder_avg = nn.Sequential(</span><br><span class="line">                    nn.Linear(args.state_latent_dim + args.action_latent_dim, args.state_latent_dim),</span><br><span class="line">                    nn.ReLU(),</span><br><span class="line">                    nn.Linear(args.state_latent_dim, self.obs_dim))</span><br><span class="line">				<span class="comment">#右下角的动作编码器</span></span><br><span class="line">                self.action_encoder = nn.Sequential(nn.Linear(self.n_actions, args.state_latent_dim * <span class="number">2</span>),</span><br><span class="line">                                                    nn.ReLU(),</span><br><span class="line">                                                    nn.Linear(args.state_latent_dim * <span class="number">2</span>, args.action_latent_dim))</span><br><span class="line">				<span class="comment">#右上角的reward解码器</span></span><br><span class="line">                self.reward_decoder_avg = nn.Sequential(</span><br><span class="line">                    nn.Linear(args.state_latent_dim + args.action_latent_dim, args.state_latent_dim),</span><br><span class="line">                    nn.ReLU(),</span><br><span class="line">                    nn.Linear(args.state_latent_dim, <span class="number">1</span>))</span><br><span class="line">            <span class="comment">#其前向传播只涉及到动作编码器</span></span><br><span class="line">            <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self</span>):</span></span><br><span class="line">                actions = th.Tensor(np.eye(self.n_actions)).to(self.args.device)</span><br><span class="line">                actions_latent_avg = self.action_encoder(actions)</span><br><span class="line">                <span class="keyword">return</span> actions_latent_avg</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#运行runner</span></span><br><span class="line">    runner.setup(scheme=scheme, groups=groups, preprocess=preprocess, mac=mac)</span><br><span class="line">    <span class="comment">#partial() 函数允许你给一个或多个参数设置固定的值，减少接下来被调用时的参数个数。</span></span><br><span class="line">    <span class="comment">#episode_buffer 实现了EpisodeBatch 实际上就是每一次的信息</span></span><br><span class="line">    	self.new_batch = partial(EpisodeBatch, scheme, groups, self.batch_size, self.episode_limit + <span class="number">1</span>,preprocess=preprocess, device=self.args.device)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Learner</span></span><br><span class="line">    <span class="comment">#from learners import REGISTRY as le_REGISTRY</span></span><br><span class="line">    learner = le_REGISTRY[args.learner](mac, buffer.scheme, logger, args)</span><br><span class="line">    </span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">RODELearner</span>:</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, mac, scheme, logger, args</span>):</span></span><br><span class="line">            self.args = args</span><br><span class="line">            self.mac = mac</span><br><span class="line">            self.logger = logger</span><br><span class="line">            self.n_agents = args.n_agents</span><br><span class="line">            self.params = <span class="built_in">list</span>(mac.parameters())</span><br><span class="line">            self.last_target_update_episode = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            self.mixer = <span class="literal">None</span></span><br><span class="line">            <span class="keyword">if</span> args.mixer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">if</span> args.mixer == <span class="string">&quot;vdn&quot;</span>:</span><br><span class="line">                    self.mixer = VDNMixer()</span><br><span class="line">                <span class="keyword">elif</span> args.mixer == <span class="string">&quot;qmix&quot;</span>:<span class="comment">#args.mixer确定为qmix</span></span><br><span class="line">                    self.mixer = QMixer(args)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">raise</span> ValueError(<span class="string">&quot;Mixer &#123;&#125; not recognised.&quot;</span>.<span class="built_in">format</span>(args.mixer))</span><br><span class="line">                self.params += <span class="built_in">list</span>(self.mixer.parameters())</span><br><span class="line">                self.target_mixer = copy.deepcopy(self.mixer)</span><br><span class="line"></span><br><span class="line">            self.role_mixer = <span class="literal">None</span></span><br><span class="line">            <span class="keyword">if</span> args.role_mixer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">if</span> args.role_mixer == <span class="string">&quot;vdn&quot;</span>:</span><br><span class="line">                    self.role_mixer = VDNMixer()</span><br><span class="line">                <span class="keyword">elif</span> args.role_mixer == <span class="string">&quot;qmix&quot;</span>::<span class="comment">#args.role_mixer确定为qmix</span></span><br><span class="line">                    self.role_mixer = QMixer(args)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">raise</span> ValueError(<span class="string">&quot;Role Mixer &#123;&#125; not recognised.&quot;</span>.<span class="built_in">format</span>(args.role_mixer))</span><br><span class="line">                self.params += <span class="built_in">list</span>(self.role_mixer.parameters())</span><br><span class="line">                self.target_role_mixer = copy.deepcopy(self.role_mixer)</span><br><span class="line"></span><br><span class="line">            self.optimiser = RMSprop(params=self.params, lr=args.lr, alpha=args.optim_alpha, eps=args.optim_eps)<span class="comment">#RMSprop优化算法起到的效果是在参数空间更为平缓的方向，会取得更大的进步（因为平缓，所以历史梯度平方和较小，对应学习下降的幅度较小），并且能够使得陡峭的方向变得平缓，从而加快训练速度。</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># a little wasteful to deepcopy (e.g. duplicates action selector), but should work for any MAC</span></span><br><span class="line">            self.target_mac = copy.deepcopy(mac)</span><br><span class="line"></span><br><span class="line">            self.log_stats_t = -self.args.learner_log_interval - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            self.role_interval = args.role_interval</span><br><span class="line">            self.device = self.args.device</span><br><span class="line"></span><br><span class="line">            self.role_action_spaces_updated = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># action encoder 动作编码器</span></span><br><span class="line">            self.action_encoder_params = <span class="built_in">list</span>(self.mac.action_encoder_params())</span><br><span class="line">            self.action_encoder_optimiser = RMSprop(params=self.action_encoder_params, lr=args.lr,alpha=args.optim_alpha, eps=args.optim_eps)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#######################################</span></span><br><span class="line">    <span class="comment">#上边都是构建网络结构 初始化一些东西</span></span><br><span class="line">    <span class="comment">#下面是训练</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#根据load_step调整 timestep</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#设置参数</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> runner.t_env &lt;= args.t_max:</span><br><span class="line">        <span class="comment"># 一次跑一个episode</span></span><br><span class="line">        episode_batch = runner.run(test_mode=<span class="literal">False</span>)</span><br><span class="line">        <span class="comment">#episode_runner.py</span></span><br><span class="line">------------------------------------------------------------------ </span><br><span class="line">episode_batch = runner.run 开始执行</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            <span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">self, test_mode=<span class="literal">False</span>, t_episode=<span class="number">0</span></span>):</span></span><br><span class="line">        		self.reset()<span class="comment">#重启环境</span></span><br><span class="line">                terminated = <span class="literal">False</span><span class="comment">#没结束</span></span><br><span class="line">        		episode_return = <span class="number">0</span></span><br><span class="line">        		self.mac.init_hidden(batch_size=self.batch_size)</span><br><span class="line">                </span><br><span class="line">                <span class="keyword">while</span> <span class="keyword">not</span> terminated:</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment">#记录信息</span></span><br><span class="line">                    </span><br><span class="line">                    </span><br><span class="line">                    <span class="comment">#将到目前为止的全部experiences传递给agents</span></span><br><span class="line">                    <span class="comment">#在此时间步骤接收batch of size为1的批中每个agent的actions </span></span><br><span class="line">                    <span class="comment"># Pass the entire batch of experiences up till now to the agents</span></span><br><span class="line">            		<span class="comment"># Receive the actions for each agent at this timestep in a batch of size 1</span></span><br><span class="line">                    actions, roles, role_avail_actions = self.mac.select_actions(self.batch, t_ep=self.t, t_env=self.t_env, test_mode=test_mode)</span><br><span class="line">------------------------------------------------------------------ </span><br><span class="line">解释select_actions开始</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">select_actions</span>(<span class="params">self,ep_batch,t_ep,t_env,bs=<span class="built_in">slice</span>(<span class="params"><span class="literal">None</span></span>),test_mode=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="comment"># Only select actions for the selected batch elements in bs</span></span><br><span class="line">        <span class="comment">#只对bs中选中的批处理元素选择action</span></span><br><span class="line">        avail_actions = ep_batch[<span class="string">&quot;avail_actions&quot;</span>][:, t_ep]</span><br><span class="line">        agent_outputs, role_outputs = self.forward(ep_batch, t_ep, test_mode=test_mode, t_env=t_env)</span><br><span class="line">        <span class="comment">#forward函数返回每个代理的q值，角色由self.selected_roles表示</span></span><br><span class="line">        <span class="comment"># the function forward returns q values of each agent, the roles are indicated by self.selected_roles</span></span><br><span class="line">------------------------------------------------------------------ </span><br><span class="line">解释forward self.mac.forward</span><br><span class="line">            		<span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, ep_batch, t, test_mode=<span class="literal">False</span>, t_env=<span class="literal">None</span></span>):</span></span><br><span class="line">        agent_inputs = self._build_inputs(ep_batch, t)</span><br><span class="line">        <span class="comment"># select roles 选择角色 角色选择器</span></span><br><span class="line">        </span><br><span class="line">        self.role_hidden_states = self.role_agent(agent_inputs, self.role_hidden_states)<span class="comment">#构建角色agent </span></span><br><span class="line">        role_outputs = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> t % self.role_interval == <span class="number">0</span>:</span><br><span class="line">            role_outputs = self.role_selector(self.role_hidden_states, self.role_latent)<span class="comment">#构建dot_roleselector 角色选择器</span></span><br><span class="line">            self.selected_roles = self.role_selector.select_role(role_outputs, test_mode=test_mode, t_env=t_env).squeeze()<span class="comment">#选择角色 ？没看懂</span></span><br><span class="line">            <span class="comment"># [bs * n_agents]</span></span><br><span class="line"> ------------------------------------------------------------------</span><br><span class="line">解释select_role</span><br><span class="line">              <span class="function"><span class="keyword">def</span> <span class="title">select_role</span>(<span class="params">self, role_qs, test_mode=<span class="literal">False</span>, t_env=<span class="literal">None</span></span>):</span></span><br><span class="line">                            self.epsilon = self.epsilon_schedule(t_env)</span><br><span class="line"></span><br><span class="line">                            <span class="keyword">if</span> test_mode:</span><br><span class="line">                                <span class="comment"># Greedy action selection only 仅测试 选最佳的</span></span><br><span class="line">                                self.epsilon = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">                            <span class="comment"># 屏蔽从选择中排除的操作</span></span><br><span class="line">                            masked_q_values = role_qs.detach().clone()</span><br><span class="line">                            </span><br><span class="line">                            random_numbers = th.rand_like(role_qs[:, <span class="number">0</span>])<span class="comment">#返回与输入相同大小的张量，该张量由区间[0,1)上均匀分布的随机数填充。</span></span><br><span class="line">                            pick_random = (random_numbers &lt; self.epsilon).long()<span class="comment">#小于epsilon作为选到的</span></span><br><span class="line">                            <span class="comment">#Categorical作用是创建以参数probs为标准的类别分布，样本是来自 “0 … K-1” 的整数，其中 K 是probs参数的长度。也就是说，按照传入的probs中给定的概率，在相应的位置处进行取样，取样返回的是该位置的整数索引。 #按照概率采样一个返回索引</span></span><br><span class="line">                            random_roles = Categorical(th.ones(role_qs.shape).<span class="built_in">float</span>().to(self.args.device)).sample().long()</span><br><span class="line"></span><br><span class="line">                            picked_roles = pick_random * random_roles + (<span class="number">1</span> - pick_random) *masked_q_values.<span class="built_in">max</span>(dim=<span class="number">1</span>)[<span class="number">1</span>]<span class="comment">#选的角色加上被屏蔽的角色</span></span><br><span class="line">                            <span class="comment"># [bs, 1]</span></span><br><span class="line">                            <span class="keyword">return</span> picked_roles</span><br><span class="line">------------------------------------------------------------------</span><br><span class="line">继续forward</span><br><span class="line">        <span class="comment"># compute individual q-values #没看懂 计算个人q-values 角色策略</span></span><br><span class="line">        self.hidden_states = self.agent(agent_inputs, self.hidden_states)<span class="comment">#建立agent</span></span><br><span class="line">        roles_q = []</span><br><span class="line">        <span class="keyword">for</span> role_i <span class="keyword">in</span> <span class="built_in">range</span>(self.n_roles):<span class="comment">#self.roles是在本类中bulid的roles</span></span><br><span class="line">            role_q = self.roles[role_i](self.hidden_states, self.action_repr)  <span class="comment"># [bs * n_agents, n_actions]</span></span><br><span class="line">            roles_q.append(role_q)</span><br><span class="line">        roles_q = th.stack(roles_q, dim=<span class="number">1</span>)  <span class="comment"># [bs*n_agents, n_roles, n_actions] 计算每个role的q</span></span><br><span class="line">        agent_outs=th.gather(roles_q,<span class="number">1</span>,self.selected_roles.unsqueeze(-<span class="number">1</span>).unsqueeze(-<span class="number">1</span>).repeat(<span class="number">1</span>, <span class="number">1</span>, self.n_actions))<span class="comment">#agent_outs 在dim=1 按照行维度上 按照selected_roles的值为索引 提取roles_q的值</span></span><br><span class="line">        <span class="comment">#得到的应该是选取的roles的对应的q值（里面有角色不同的actions对应的q）</span></span><br><span class="line">        <span class="comment"># [bs * n_agents, 1, n_actions]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> agent_outs.view(ep_batch.batch_size, self.n_agents, -<span class="number">1</span>), \</span><br><span class="line">            (<span class="literal">None</span> <span class="keyword">if</span> role_outputs <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> role_outputs.view(ep_batch.batch_size, self.n_agents, -<span class="number">1</span>))</span><br><span class="line">------------------------------------------------------------------</span><br><span class="line">继续select_actions    </span><br><span class="line">		<span class="comment">#过滤掉对所选角色不可行的操作 </span></span><br><span class="line">        <span class="comment"># filter out actions infeasible for selected roles; self.selected_roles [bs*n_agents]</span></span><br><span class="line">        <span class="comment"># self.role_action_spaces [n_roles, n_actions]</span></span><br><span class="line">        role_avail_actions =th.gather(self.role_action_spaces.unsqueeze(<span class="number">0</span>).repeat(self.n_agents, <span class="number">1</span>,<span class="number">1</span>),dim=<span class="number">1</span>,index=self.selected_roles.unsqueeze(-<span class="number">1</span>).unsqueeze(-<span class="number">1</span>).repeat(<span class="number">1</span>,<span class="number">1</span>,self.n_actions).long()).squeeze()<span class="comment">#角色可选操作结果 按照后面选择的roles来从role_action_spaces中选出对应能使用的actions</span></span><br><span class="line">        role_avail_actions = role_avail_actions.<span class="built_in">int</span>().view(ep_batch.batch_size, self.n_agents, -<span class="number">1</span>)</span><br><span class="line">        chosen_actions = self.action_selector.select_action(agent_outputs[bs], avail_actions[bs],role_avail_actions[bs], t_env, test_mode=test_mode)</span><br><span class="line">        </span><br><span class="line">------------------------------------------------------------------</span><br><span class="line">开始 self.action_selector.select_action</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">select_action</span>(<span class="params">self, agent_inputs, avail_actions, role_avail_actions, t_env, test_mode=<span class="literal">False</span></span>):</span><span class="comment">#？没看懂</span></span><br><span class="line"></span><br><span class="line">    	<span class="comment"># Assuming agent_inputs is a batch of Q-Values for each agent bav</span></span><br><span class="line">        <span class="comment">#假设agent_inputs是每个agent bav的一批q - value</span></span><br><span class="line">    	self.epsilon = self.schedule.<span class="built_in">eval</span>(t_env)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> test_mode:</span><br><span class="line">            <span class="comment"># Greedy action selection only</span></span><br><span class="line">            self.epsilon = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># mask actions that are excluded from selection 屏蔽从selection中排除的actions</span></span><br><span class="line">        masked_q_values = agent_inputs.clone()</span><br><span class="line">        d_avail_actions = avail_actions * role_avail_actions<span class="comment">#可用动作</span></span><br><span class="line">        masked_q_values[d_avail_actions == <span class="number">0.0</span>] = -<span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>)  <span class="comment"># should never be selected!</span></span><br><span class="line"></span><br><span class="line">        random_numbers = th.rand_like(agent_inputs[:, :, <span class="number">0</span>])</span><br><span class="line">        pick_random = (random_numbers &lt; self.epsilon).long()</span><br><span class="line">        random_actions = Categorical(avail_actions.<span class="built_in">float</span>()).sample().long()</span><br><span class="line"></span><br><span class="line">        picked_actions = pick_random * random_actions + (<span class="number">1</span> - pick_random) * masked_q_values.<span class="built_in">max</span>(dim=<span class="number">2</span>)[<span class="number">1</span>]</span><br><span class="line">        ind = th.gather(avail_actions, dim=<span class="number">2</span>, index=picked_actions.unsqueeze(<span class="number">2</span>)) &gt; <span class="number">0.99</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> ind.<span class="built_in">all</span>():</span><br><span class="line">            <span class="comment"># print(&quot;&gt;&gt;&gt; Action Selection Error&quot;)</span></span><br><span class="line">            ind = ind.squeeze().long()</span><br><span class="line">            picked_actions = picked_actions * ind + (<span class="number">1</span> - ind) * random_actions</span><br><span class="line">            <span class="keyword">return</span> picked_actions</span><br><span class="line">    </span><br><span class="line">结束select_actions  </span><br><span class="line">------------------------------------------------------------------</span><br><span class="line">self.batch.update(&#123;<span class="string">&quot;role_avail_actions&quot;</span>: role_avail_actions.tolist()&#125;, ts=self.t)<span class="comment">#更新</span></span><br><span class="line">    </span><br><span class="line">reward, terminated, env_info = self.env.step(actions[<span class="number">0</span>])<span class="comment">#环境更新一步 获得 reward, terminated, env_info</span></span><br><span class="line">episode_return += reward</span><br><span class="line"></span><br><span class="line">post_transition_data = &#123;</span><br><span class="line">    <span class="string">&quot;actions&quot;</span>: actions,</span><br><span class="line">    <span class="string">&quot;roles&quot;</span>: roles,</span><br><span class="line">    <span class="string">&quot;role_avail_actions&quot;</span>: role_avail_actions,</span><br><span class="line">    <span class="string">&quot;reward&quot;</span>: [(reward,)],</span><br><span class="line">    <span class="string">&quot;terminated&quot;</span>: [(terminated != env_info.get(<span class="string">&quot;episode_limit&quot;</span>, <span class="literal">False</span>),)],</span><br><span class="line">&#125;</span><br><span class="line">self.batch.update(post_transition_data, ts=self.t)<span class="comment">#更新信息</span></span><br><span class="line">self.t += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">----------------------------------------------------------------</span><br><span class="line">上面实际上episode_runner的run总过程为</span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> terminated:</span><br><span class="line">            pre_transition_data = &#123;</span><br><span class="line">                <span class="string">&quot;state&quot;</span>: [self.env.get_state()],</span><br><span class="line">                <span class="string">&quot;avail_actions&quot;</span>: [self.env.get_avail_actions()],</span><br><span class="line">                <span class="string">&quot;obs&quot;</span>: [self.env.get_obs()]</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            self.batch.update(pre_transition_data, ts=self.t)<span class="comment">#先更新一次当前的状态 obs 可用动作</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">#将到目前为止的全部experiences传递给agents</span></span><br><span class="line">            <span class="comment">#在此时间步骤接收batch of size为1的批中每个agent的actions </span></span><br><span class="line">            <span class="comment"># Pass the entire batch of experiences up till now to the agents</span></span><br><span class="line">            <span class="comment"># Receive the actions for each agent at this timestep in a batch of size 1</span></span><br><span class="line">            actions, roles, role_avail_actions = self.mac.select_actions(self.batch, t_ep=self.t,t_env=self.t_env, test_mode=test_mode)<span class="comment">#选这一次动作</span></span><br><span class="line">            </span><br><span class="line">            self.batch.update(&#123;<span class="string">&quot;role_avail_actions&quot;</span>: role_avail_actions.tolist()&#125;, ts=self.t)</span><br><span class="line">            <span class="comment">#更新本次角色可用动作</span></span><br><span class="line">            </span><br><span class="line">            reward, terminated, env_info = self.env.step(actions[<span class="number">0</span>])<span class="comment">#走一次</span></span><br><span class="line">            episode_return += reward</span><br><span class="line">            post_transition_data = &#123;</span><br><span class="line">                <span class="string">&quot;actions&quot;</span>: actions,</span><br><span class="line">                <span class="string">&quot;roles&quot;</span>: roles,</span><br><span class="line">                <span class="string">&quot;role_avail_actions&quot;</span>: role_avail_actions,</span><br><span class="line">                <span class="string">&quot;reward&quot;</span>: [(reward,)],</span><br><span class="line">                <span class="string">&quot;terminated&quot;</span>: [(terminated != env_info.get(<span class="string">&quot;episode_limit&quot;</span>, <span class="literal">False</span>),)],</span><br><span class="line">            &#125;<span class="comment">#获得选择一次动作后得信息</span></span><br><span class="line">            self.batch.update(post_transition_data, ts=self.t)</span><br><span class="line">            self.t += <span class="number">1</span></span><br><span class="line">----------------------------------------------------------------</span><br><span class="line">terminated结束后</span><br><span class="line"> <span class="comment"># Select actions in the last stored state 选择上次存储状态下的actions</span></span><br><span class="line">        actions, roles, role_avail_actions = self.mac.select_actions(self.batch, t_ep=self.t, t_env=self.t_env, test_mode=test_mode)</span><br><span class="line">        self.batch.update(&#123;<span class="string">&quot;actions&quot;</span>: actions, <span class="string">&quot;roles&quot;</span>: roles, <span class="string">&quot;role_avail_actions&quot;</span>: role_avail_actions&#125;, ts=self.t)</span><br><span class="line"><span class="comment">#日志操作</span></span><br><span class="line">        cur_stats = self.test_stats <span class="keyword">if</span> test_mode <span class="keyword">else</span> self.train_stats</span><br><span class="line">        cur_returns = self.test_returns <span class="keyword">if</span> test_mode <span class="keyword">else</span> self.train_returns</span><br><span class="line">        log_prefix = <span class="string">&quot;test_&quot;</span> <span class="keyword">if</span> test_mode <span class="keyword">else</span> <span class="string">&quot;&quot;</span></span><br><span class="line">        cur_stats.update(&#123;k: cur_stats.get(k, <span class="number">0</span>) + env_info.get(k, <span class="number">0</span>) <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">set</span>(cur_stats) | <span class="built_in">set</span>(env_info)&#125;)</span><br><span class="line">        cur_stats[<span class="string">&quot;n_episodes&quot;</span>] = <span class="number">1</span> + cur_stats.get(<span class="string">&quot;n_episodes&quot;</span>, <span class="number">0</span>)</span><br><span class="line">        cur_stats[<span class="string">&quot;ep_length&quot;</span>] = self.t + cur_stats.get(<span class="string">&quot;ep_length&quot;</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> test_mode:</span><br><span class="line">            self.t_env += self.t</span><br><span class="line">        cur_returns.append(episode_return)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> test_mode <span class="keyword">and</span> (<span class="built_in">len</span>(self.test_returns) == self.args.test_nepisode):</span><br><span class="line">            self._log(cur_returns, cur_stats, log_prefix)</span><br><span class="line">        <span class="keyword">elif</span> self.t_env - self.log_train_stats_t &gt;= self.args.runner_log_interval:</span><br><span class="line">            self._log(cur_returns, cur_stats, log_prefix)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">hasattr</span>(self.mac.action_selector, <span class="string">&quot;epsilon&quot;</span>):</span><br><span class="line">                self.logger.log_stat(<span class="string">&quot;epsilon&quot;</span>, self.mac.action_selector.epsilon, self.t_env)</span><br><span class="line">            self.log_train_stats_t = self.t_env</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.batch</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">episode_batch = runner.run 执行完成</span><br><span class="line">------------------------------------------------------------------------------</span><br><span class="line">跑完了一个batch的episode</span><br><span class="line"></span><br><span class="line">        buffer.insert_episode_batch(episode_batch)<span class="comment">#记录</span></span><br><span class="line">        <span class="keyword">if</span> buffer.can_sample(args.batch_size):<span class="comment"># self.episodes_in_buffer &gt;= batch_size</span></span><br><span class="line">            episode_sample = buffer.sample(args.batch_size)</span><br><span class="line">				ep_ids = np.random.choice(self.episodes_in_buffer, batch_size, replace=<span class="literal">False</span>)</span><br><span class="line">            <span class="comment"># 截断batch来填充timesteps</span></span><br><span class="line">            max_ep_t = episode_sample.max_t_filled()</span><br><span class="line">            episode_sample = episode_sample[:, :max_ep_t]</span><br><span class="line"></span><br><span class="line">            learner.train(episode_sample, runner.t_env, episode)</span><br><span class="line">            </span><br><span class="line">------------------------------------------------------------------------------</span><br><span class="line"> learner.train开始训练  </span><br><span class="line">    </span><br><span class="line">            </span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">self, batch: EpisodeBatch, t_env: <span class="built_in">int</span>, episode_num: <span class="built_in">int</span></span>):</span></span><br><span class="line">        <span class="comment"># 得到相关的变量</span></span><br><span class="line">        rewards = batch[<span class="string">&quot;reward&quot;</span>][:, :-<span class="number">1</span>]</span><br><span class="line">        actions = batch[<span class="string">&quot;actions&quot;</span>][:, :-<span class="number">1</span>]</span><br><span class="line">        terminated = batch[<span class="string">&quot;terminated&quot;</span>][:, :-<span class="number">1</span>].<span class="built_in">float</span>()</span><br><span class="line">        mask = batch[<span class="string">&quot;filled&quot;</span>][:, :-<span class="number">1</span>].<span class="built_in">float</span>()</span><br><span class="line">        mask[:, <span class="number">1</span>:] = mask[:, <span class="number">1</span>:] * (<span class="number">1</span> - terminated[:, :-<span class="number">1</span>])</span><br><span class="line">        avail_actions = batch[<span class="string">&quot;avail_actions&quot;</span>]</span><br><span class="line">        <span class="comment"># role_avail_actions = batch[&quot;role_avail_actions&quot;]</span></span><br><span class="line">        roles_shape_o = batch[<span class="string">&quot;roles&quot;</span>][:, :-<span class="number">1</span>].shape</span><br><span class="line">        role_at = <span class="built_in">int</span>(np.ceil(roles_shape_o[<span class="number">1</span>] / self.role_interval))</span><br><span class="line">        role_t = role_at * self.role_interval</span><br><span class="line"></span><br><span class="line">        roles_shape = <span class="built_in">list</span>(roles_shape_o)</span><br><span class="line">        roles_shape[<span class="number">1</span>] = role_t</span><br><span class="line">        roles = th.zeros(roles_shape).to(self.device)</span><br><span class="line">        roles[:, :roles_shape_o[<span class="number">1</span>]] = batch[<span class="string">&quot;roles&quot;</span>][:, :-<span class="number">1</span>]</span><br><span class="line">        roles = roles.view(batch.batch_size, role_at, self.role_interval, self.n_agents, -<span class="number">1</span>)[:, :, <span class="number">0</span>]</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># 计算q值估计 Calculate estimated Q-Values</span></span><br><span class="line">        mac_out = []</span><br><span class="line">        role_out = []</span><br><span class="line">        self.mac.init_hidden(batch.batch_size)<span class="comment">#初始化隐层</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(batch.max_seq_length):</span><br><span class="line">            agent_outs, role_outs = self.mac.forward(batch, t=t)<span class="comment">#forward</span></span><br><span class="line">            mac_out.append(agent_outs)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> t % self.role_interval == <span class="number">0</span> <span class="keyword">and</span> t &lt; batch.max_seq_length - <span class="number">1</span>:</span><br><span class="line">                role_out.append(role_outs)</span><br><span class="line">        mac_out = th.stack(mac_out, dim=<span class="number">1</span>)  <span class="comment"># Concat over time</span></span><br><span class="line">        role_out = th.stack(role_out, dim=<span class="number">1</span>)  <span class="comment"># Concat over time</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Pick the Q-Values for the actions taken by each agent 计算为每个agent采取的actions的q值</span></span><br><span class="line">        chosen_action_qvals = th.gather(mac_out[:, :-<span class="number">1</span>], dim=<span class="number">3</span>, index=actions).squeeze(<span class="number">3</span>)  <span class="comment"># Remove the last dim</span></span><br><span class="line">        <span class="comment">#在dim=3维度上 返回索引为actions的mac_out的值 mac_out返回的是agent选的role的q值 那么此处就是agent选的role之后选的actions的q值</span></span><br><span class="line">        </span><br><span class="line">        chosen_role_qvals = th.gather(role_out, dim=<span class="number">3</span>, index=roles.long()).squeeze(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculate the Q-Values necessary for the target 计算target所需的q值</span></span><br><span class="line">        target_mac_out = []</span><br><span class="line">        target_role_out = []</span><br><span class="line"></span><br><span class="line">        <span class="comment">#self.target_mac = copy.deepcopy(mac)</span></span><br><span class="line">    	<span class="comment">#得到target_mac的值</span></span><br><span class="line">        self.target_mac.init_hidden(batch.batch_size)</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(batch.max_seq_length):</span><br><span class="line">            target_agent_outs, target_role_outs = self.target_mac.forward(batch, t=t)</span><br><span class="line">            target_mac_out.append(target_agent_outs)</span><br><span class="line">            <span class="keyword">if</span> t % self.role_interval == <span class="number">0</span> <span class="keyword">and</span> t &lt; batch.max_seq_length - <span class="number">1</span>:</span><br><span class="line">                target_role_out.append(target_role_outs)</span><br><span class="line"></span><br><span class="line">        target_role_out.append(th.zeros(batch.batch_size, self.n_agents, self.mac.n_roles).to(self.device))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># We don&#x27;t need the first timesteps Q-Value estimate for calculating targets</span></span><br><span class="line">        <span class="comment">#我们不需要第一时间步的q值估计来计算目标</span></span><br><span class="line">        target_mac_out = th.stack(target_mac_out[<span class="number">1</span>:], dim=<span class="number">1</span>)  <span class="comment"># Concat across time</span></span><br><span class="line">        target_role_out = th.stack(target_role_out[<span class="number">1</span>:], dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Mask out unavailable actions 屏蔽不可用的actions</span></span><br><span class="line">        target_mac_out[avail_actions[:, <span class="number">1</span>:] == <span class="number">0</span>] = -<span class="number">9999999</span></span><br><span class="line">        <span class="comment"># target_mac_out[role_avail_actions[:, 1:] == 0] = -9999999</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Max over target Q-Values 最大值超过 target q值</span></span><br><span class="line">        <span class="keyword">if</span> self.args.double_q:</span><br><span class="line">            <span class="comment"># Get actions that maximise live Q (for double q-learning)</span></span><br><span class="line">            <span class="comment">#获得最大化Q值的actions(对于双Q学习) 这里怀疑是loss由两个q-learning 组成</span></span><br><span class="line">            </span><br><span class="line">            mac_out_detach = mac_out.clone().detach()<span class="comment">#复制mac_out</span></span><br><span class="line">            mac_out_detach[avail_actions == <span class="number">0</span>] = -<span class="number">9999999</span><span class="comment">#屏蔽不可用的动作</span></span><br><span class="line">            <span class="comment"># mac_out_detach[role_avail_actions == 0] = -9999999</span></span><br><span class="line">            cur_max_actions = mac_out_detach[:, <span class="number">1</span>:].<span class="built_in">max</span>(dim=<span class="number">3</span>, keepdim=<span class="literal">True</span>)[<span class="number">1</span>]<span class="comment">#max操作 得到最大值索引 mac_out在第三维度中的最大的值</span></span><br><span class="line">            target_max_qvals = th.gather(target_mac_out, <span class="number">3</span>, cur_max_actions).squeeze(<span class="number">3</span>)<span class="comment">#gather通过cur_max_actions提取的最大值的索引 在dim=3维度中 提取target_mac_out的数据</span></span><br><span class="line">            <span class="comment">#这里用的是mac_out的最大索引提取的targetmac_out的值</span></span><br><span class="line">            </span><br><span class="line"></span><br><span class="line">            role_out_detach = role_out.clone().detach()</span><br><span class="line">            role_out_detach = th.cat([role_out_detach[:, <span class="number">1</span>:], role_out_detach[:, <span class="number">0</span>:<span class="number">1</span>]], dim=<span class="number">1</span>)</span><br><span class="line">            cur_max_roles = role_out_detach.<span class="built_in">max</span>(dim=<span class="number">3</span>, keepdim=<span class="literal">True</span>)[<span class="number">1</span>]</span><br><span class="line">            target_role_max_qvals = th.gather(target_role_out, <span class="number">3</span>, cur_max_roles).squeeze(<span class="number">3</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            target_max_qvals = target_mac_out.<span class="built_in">max</span>(dim=<span class="number">3</span>)[<span class="number">0</span>]</span><br><span class="line">            target_role_max_qvals = target_role_out.<span class="built_in">max</span>(dim=<span class="number">3</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Mix</span></span><br><span class="line">        <span class="keyword">if</span> self.mixer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment">#选定chosen_action_qvals 同时得到target</span></span><br><span class="line">            <span class="comment">#选择的action 的q值</span></span><br><span class="line">            </span><br><span class="line">            chosen_action_qvals = self.mixer(chosen_action_qvals, batch[<span class="string">&quot;state&quot;</span>][:, :-<span class="number">1</span>])</span><br><span class="line">            target_max_qvals = self.target_mixer(target_max_qvals, batch[<span class="string">&quot;state&quot;</span>][:, <span class="number">1</span>:])</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">if</span> self.role_mixer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment">#选定chosen_role_qvals 同时得到target</span></span><br><span class="line">            <span class="comment">#选择的role 的q值</span></span><br><span class="line">            </span><br><span class="line">            state_shape_o = batch[<span class="string">&quot;state&quot;</span>][:, :-<span class="number">1</span>].shape</span><br><span class="line">            state_shape = <span class="built_in">list</span>(state_shape_o)</span><br><span class="line">            state_shape[<span class="number">1</span>] = role_t</span><br><span class="line">            role_states = th.zeros(state_shape).to(self.device)</span><br><span class="line">            role_states[:, :state_shape_o[<span class="number">1</span>]] = batch[<span class="string">&quot;state&quot;</span>][:, :-<span class="number">1</span>].detach().clone()</span><br><span class="line">            </span><br><span class="line">            role_states = role_states.view(batch.batch_size, role_at,</span><br><span class="line">                                           self.role_interval, -<span class="number">1</span>)[:, :, <span class="number">0</span>]</span><br><span class="line">            chosen_role_qvals = self.role_mixer(chosen_role_qvals, role_states)</span><br><span class="line">            role_states = th.cat([role_states[:, <span class="number">1</span>:], role_states[:, <span class="number">0</span>:<span class="number">1</span>]], dim=<span class="number">1</span>)</span><br><span class="line">            target_role_max_qvals = self.target_role_mixer(target_role_max_qvals, role_states)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculate 1-step Q-Learning targets</span></span><br><span class="line">        <span class="comment">#计算1步 q-learning targets？</span></span><br><span class="line">        </span><br><span class="line">        targets = rewards + self.args.gamma * (<span class="number">1</span> - terminated) * target_max_qvals</span><br><span class="line">        rewards_shape = <span class="built_in">list</span>(rewards.shape)</span><br><span class="line">        rewards_shape[<span class="number">1</span>] = role_t</span><br><span class="line">        role_rewards = th.zeros(rewards_shape).to(self.device)</span><br><span class="line">        role_rewards[:, :rewards.shape[<span class="number">1</span>]] = rewards.detach().clone()</span><br><span class="line">        role_rewards = role_rewards.view(batch.batch_size, role_at,</span><br><span class="line">                                         self.role_interval).<span class="built_in">sum</span>(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># role_terminated</span></span><br><span class="line">        </span><br><span class="line">        terminated_shape_o = terminated.shape</span><br><span class="line">        terminated_shape = <span class="built_in">list</span>(terminated_shape_o)</span><br><span class="line">        terminated_shape[<span class="number">1</span>] = role_t</span><br><span class="line">        role_terminated = th.zeros(terminated_shape).to(self.device)</span><br><span class="line">        role_terminated[:, :terminated_shape_o[<span class="number">1</span>]] = terminated.detach().clone()</span><br><span class="line">        role_terminated = role_terminated.view(batch.batch_size, role_at, self.role_interval).<span class="built_in">sum</span>(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># role_terminated？</span></span><br><span class="line">        role_targets = role_rewards + self.args.gamma * (<span class="number">1</span> - role_terminated) * target_role_max_qvals</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Td-error</span></span><br><span class="line">        td_error = (chosen_action_qvals - targets.detach())</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#td_error中 chosen_action_qvals 是agent选的role之后选的actions的q值再经过qmix混合后得到的全局的q_tot</span></span><br><span class="line">       	</span><br><span class="line">     </span><br></pre></td></tr></table></figure>
<p><img src="https://i.loli.net/2021/07/18/3T1RMwCdmQHzAn8.png" alt="image-20210718120752326"></p>
<p>其中chosen_action_qvals为Q_tot 而targets为Qtarget</p>
<p>​        </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br></pre></td><td class="code"><pre><span class="line">    role_td_error = (chosen_role_qvals - role_targets.detach())</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#role_td_error中的chosen_role_qvals是role_outs集成得到的role_out 变形为chosen_role_qvals 再经过mix得到的 其中role_outs是self.role_selector构建的角色选择器 </span></span><br><span class="line"></span><br><span class="line">    mask = mask.expand_as(td_error)</span><br><span class="line">    mask_shape = <span class="built_in">list</span>(mask.shape)</span><br><span class="line">    mask_shape[<span class="number">1</span>] = role_t</span><br><span class="line">    role_mask = th.zeros(mask_shape).to(self.device)</span><br><span class="line">    role_mask[:, :mask.shape[<span class="number">1</span>]] = mask.detach().clone()</span><br><span class="line">    role_mask = role_mask.view(batch.batch_size, role_at, self.role_interval, -<span class="number">1</span>)[:, :, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 0-out the targets that came from padded data</span></span><br><span class="line">    masked_td_error = td_error * mask</span><br><span class="line">    masked_role_td_error = role_td_error * role_mask</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Normal L2 loss, take mean over actual data</span></span><br><span class="line">    loss = (masked_td_error ** <span class="number">2</span>).<span class="built_in">sum</span>() / mask.<span class="built_in">sum</span>()</span><br><span class="line">    role_loss = (masked_role_td_error ** <span class="number">2</span>).<span class="built_in">sum</span>() / role_mask.<span class="built_in">sum</span>()</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#两个loss相加</span></span><br><span class="line">    loss += role_loss</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Optimise</span></span><br><span class="line">    self.optimiser.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    grad_norm = th.nn.utils.clip_grad_norm_(self.params, self.args.grad_norm_clip)</span><br><span class="line">    self.optimiser.step()</span><br><span class="line"></span><br><span class="line">    pred_obs_loss = <span class="literal">None</span></span><br><span class="line">    pred_r_loss = <span class="literal">None</span></span><br><span class="line">    pred_grad_norm = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> self.role_action_spaces_updated:</span><br><span class="line">        <span class="comment"># train action encoder</span></span><br><span class="line">        <span class="comment"># 训练动作编码器</span></span><br><span class="line">        no_pred = []</span><br><span class="line">        r_pred = []</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(batch.max_seq_length):</span><br><span class="line">            no_preds, r_preds = self.mac.action_repr_forward(batch, t=t)</span><br><span class="line">----------------------------------------------------------------------</span><br><span class="line">action_repr_forward开始</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">action_repr_forward</span>(<span class="params">self, ep_batch, t</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.action_encoder.predict(ep_batch[<span class="string">&quot;obs&quot;</span>][:, t], ep_batch[<span class="string">&quot;actions_onehot&quot;</span>][:, t])</span><br><span class="line">----------------------------------------------------------------------</span><br><span class="line">predict开始</span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, obs, actions</span>):</span></span><br><span class="line">        <span class="comment"># used in learners (for training)</span></span><br><span class="line">        other_actions = self.other_actions(actions) <span class="comment">#是所有n_agents的其他的n_agents的action的拼接</span></span><br><span class="line">----------------------------------------------------------------------</span><br><span class="line">other_actions开始 </span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">other_actions</span>(<span class="params">self, actions</span>):</span></span><br><span class="line">        <span class="comment"># actions: [bs, n_agents, n_actions]</span></span><br><span class="line">        <span class="keyword">assert</span> actions.shape[<span class="number">1</span>] == self.n_agents</span><br><span class="line"></span><br><span class="line">        other_actions = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.n_agents):</span><br><span class="line">            _other_actions  = []</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(self.n_agents):</span><br><span class="line">                <span class="keyword">if</span> i != j:</span><br><span class="line">                    _other_actions.append(actions[:, j])</span><br><span class="line">            _other_actions = th.cat(_other_actions, dim=-<span class="number">1</span>)</span><br><span class="line">            other_actions.append(_other_actions)</span><br><span class="line"></span><br><span class="line">        other_actions = th.stack(other_actions, dim=<span class="number">1</span>).contiguous().view(-<span class="number">1</span>, (self.n_agents - <span class="number">1</span>) * self.n_actions)</span><br><span class="line">        <span class="comment">#stack()沿着一个新维度对输入张量序列进行连接。 序列中所有的张量都应该为相同形状。</span></span><br><span class="line">        <span class="comment">#cat() 在给定维度上对输入的张量序列seq 进行连接操作。</span></span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> other_actions</span><br><span class="line">        </span><br><span class="line">   other_actions结束</span><br><span class="line">----------------------------------------------------------------------        </span><br><span class="line">predict继续</span><br><span class="line">        </span><br><span class="line">        obs_reshaped = obs.contiguous().view(-<span class="number">1</span>, self.obs_dim)</span><br><span class="line">        inputs = th.cat([obs_reshaped, other_actions], dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># average</span></span><br><span class="line">        <span class="comment">#将obs和action分别进入编码器</span></span><br><span class="line">        obs_latent_avg = self.obs_encoder_avg(inputs)</span><br><span class="line">        actions = actions.contiguous().view(-<span class="number">1</span>, self.n_actions)</span><br><span class="line">        action_latent_avg = self.action_encoder(actions)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="comment">#两者拼接</span></span><br><span class="line">        pred_avg_input = th.cat([obs_latent_avg, action_latent_avg], dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="comment">#拼接后分别经过reward解码器和obs解码器得到结果</span></span><br><span class="line">        no_pred_avg = self.obs_decoder_avg(pred_avg_input)</span><br><span class="line">        r_pred_avg = self.reward_decoder_avg(pred_avg_input)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> no_pred_avg.view(-<span class="number">1</span>, self.n_agents, self.obs_dim), r_pred_avg.view(-<span class="number">1</span>, self.n_agents, <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">predict结束</span><br><span class="line">action_repr_forward结束</span><br><span class="line">----------------------------------------------------------------------</span><br><span class="line">            </span><br><span class="line">            no_pred.append(no_preds)<span class="comment">#obs</span></span><br><span class="line">            r_pred.append(r_preds)<span class="comment">#reward</span></span><br><span class="line">        no_pred = th.stack(no_pred, dim=<span class="number">1</span>)[:, :-<span class="number">1</span>]  <span class="comment"># Concat over time</span></span><br><span class="line">        r_pred = th.stack(r_pred, dim=<span class="number">1</span>)[:, :-<span class="number">1</span>]</span><br><span class="line">        no = batch[<span class="string">&quot;obs&quot;</span>][:, <span class="number">1</span>:].detach().clone()<span class="comment">#batch的obs的复制</span></span><br><span class="line">        repeated_rewards = batch[<span class="string">&quot;reward&quot;</span>][:, :-<span class="number">1</span>].detach().clone().unsqueeze(<span class="number">2</span>).repeat(<span class="number">1</span>, <span class="number">1</span>, self.n_agents, <span class="number">1</span>)<span class="comment">#batch的reward的复制</span></span><br><span class="line">		</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#obs和reward的loss</span></span><br><span class="line">        pred_obs_loss = th.sqrt(((no_pred - no) ** <span class="number">2</span>).<span class="built_in">sum</span>(dim=-<span class="number">1</span>)).mean()</span><br><span class="line">        pred_r_loss = ((r_pred - repeated_rewards) ** <span class="number">2</span>).mean()</span><br><span class="line"></span><br><span class="line">        <span class="comment">#结合起来 all=obs+10*reward</span></span><br><span class="line">        pred_loss = pred_obs_loss + <span class="number">10</span> * pred_r_loss</span><br><span class="line">        </span><br><span class="line">        self.action_encoder_optimiser.zero_grad()</span><br><span class="line">        pred_loss.backward()</span><br><span class="line">        pred_grad_norm = th.nn.utils.clip_grad_norm_(self.action_encoder_params, self.args.grad_norm_clip)</span><br><span class="line">        self.action_encoder_optimiser.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> t_env &gt; self.args.role_action_spaces_update_start:</span><br><span class="line">            self.mac.update_role_action_spaces()</span><br><span class="line">            <span class="keyword">if</span> <span class="string">&#x27;noar&#x27;</span> <span class="keyword">in</span> self.args.mac:</span><br><span class="line">                self.target_mac.role_selector.update_roles(self.mac.n_roles)</span><br><span class="line">            self.role_action_spaces_updated = <span class="literal">False</span></span><br><span class="line">            self._update_targets()</span><br><span class="line">            self.last_target_update_episode = episode_num</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (episode_num - self.last_target_update_episode) / self.args.target_update_interval &gt;= <span class="number">1.0</span>:</span><br><span class="line">        self._update_targets()</span><br><span class="line">        self.last_target_update_episode = episode_num</span><br><span class="line">	</span><br><span class="line">    <span class="comment">#日志</span></span><br><span class="line">    <span class="keyword">if</span> t_env - self.log_stats_t &gt;= self.args.learner_log_interval:</span><br><span class="line">        self.logger.log_stat(<span class="string">&quot;loss&quot;</span>, (loss - role_loss).item(), t_env)</span><br><span class="line">        self.logger.log_stat(<span class="string">&quot;role_loss&quot;</span>, role_loss.item(), t_env)</span><br><span class="line">        self.logger.log_stat(<span class="string">&quot;grad_norm&quot;</span>, grad_norm, t_env)</span><br><span class="line">        <span class="keyword">if</span> pred_obs_loss <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.logger.log_stat(<span class="string">&quot;pred_obs_loss&quot;</span>, pred_obs_loss.item(), t_env)</span><br><span class="line">            self.logger.log_stat(<span class="string">&quot;pred_r_loss&quot;</span>, pred_r_loss.item(), t_env)</span><br><span class="line">            self.logger.log_stat(<span class="string">&quot;action_encoder_grad_norm&quot;</span>, pred_grad_norm, t_env)</span><br><span class="line">        mask_elems = mask.<span class="built_in">sum</span>().item()</span><br><span class="line">        self.logger.log_stat(<span class="string">&quot;td_error_abs&quot;</span>, (masked_td_error.<span class="built_in">abs</span>().<span class="built_in">sum</span>().item() / mask_elems), t_env)</span><br><span class="line">        self.logger.log_stat(<span class="string">&quot;q_taken_mean&quot;</span>,</span><br><span class="line">                             (chosen_action_qvals * mask).<span class="built_in">sum</span>().item() / (mask_elems * self.args.n_agents), t_env)</span><br><span class="line">        self.logger.log_stat(<span class="string">&quot;role_q_taken_mean&quot;</span>,</span><br><span class="line">                             (chosen_role_qvals * role_mask).<span class="built_in">sum</span>().item() / (role_mask.<span class="built_in">sum</span>().item() * self.args.n_agents), t_env)</span><br><span class="line">        self.logger.log_stat(<span class="string">&quot;target_mean&quot;</span>, (targets * mask).<span class="built_in">sum</span>().item() / (mask_elems * self.args.n_agents),</span><br><span class="line">                             t_env)</span><br><span class="line">        self.log_stats_t = t_env</span><br></pre></td></tr></table></figure>
<pre><code>Arguments: ([
   array([0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
   0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]), 
   array([0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1.,
   0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0.]), 
   array([1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0.,
   1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1.])],)
</code></pre><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Arguments: ([</span><br><span class="line">	   array([1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,</span><br><span class="line">       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]), </span><br><span class="line">       array([1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1.,</span><br><span class="line">       0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0.]), </span><br><span class="line">       array([1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0.,</span><br><span class="line">       1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1.])],)</span><br></pre></td></tr></table></figure>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/09/04/Qmix%20realize%20in%20win10/" rel="prev" title="qmix realize in win10">
                  <i class="fa fa-chevron-left"></i> qmix realize in win10
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  




  





</body>
</html>
